{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":26154,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":22015}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-26T06:19:13.413763Z","iopub.execute_input":"2024-06-26T06:19:13.414127Z","iopub.status.idle":"2024-06-26T06:19:13.770250Z","shell.execute_reply.started":"2024-06-26T06:19:13.414095Z","shell.execute_reply":"2024-06-26T06:19:13.769388Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/lmsys-chatbot-arena/sample_submission.csv\n/kaggle/input/lmsys-chatbot-arena/train.csv\n/kaggle/input/lmsys-chatbot-arena/test.csv\n/kaggle/input/gemma/transformers/1.1-7b-it/1/model.safetensors.index.json\n/kaggle/input/gemma/transformers/1.1-7b-it/1/model-00003-of-00004.safetensors\n/kaggle/input/gemma/transformers/1.1-7b-it/1/config.json\n/kaggle/input/gemma/transformers/1.1-7b-it/1/model-00001-of-00004.safetensors\n/kaggle/input/gemma/transformers/1.1-7b-it/1/tokenizer.json\n/kaggle/input/gemma/transformers/1.1-7b-it/1/tokenizer_config.json\n/kaggle/input/gemma/transformers/1.1-7b-it/1/model-00004-of-00004.safetensors\n/kaggle/input/gemma/transformers/1.1-7b-it/1/special_tokens_map.json\n/kaggle/input/gemma/transformers/1.1-7b-it/1/.gitattributes\n/kaggle/input/gemma/transformers/1.1-7b-it/1/model-00002-of-00004.safetensors\n/kaggle/input/gemma/transformers/1.1-7b-it/1/tokenizer.model\n/kaggle/input/gemma/transformers/1.1-7b-it/1/generation_config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\ntrain_df = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:19:26.767695Z","iopub.execute_input":"2024-06-26T06:19:26.768191Z","iopub.status.idle":"2024-06-26T06:19:30.161182Z","shell.execute_reply.started":"2024-06-26T06:19:26.768160Z","shell.execute_reply":"2024-06-26T06:19:30.160143Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"       id             model_a              model_b  \\\n0   30192  gpt-4-1106-preview           gpt-4-0613   \n1   53567           koala-13b           gpt-4-0613   \n2   65089  gpt-3.5-turbo-0613       mistral-medium   \n3   96401    llama-2-13b-chat  mistral-7b-instruct   \n4  198779           koala-13b   gpt-3.5-turbo-0314   \n\n                                              prompt  \\\n0  [\"Is it morally right to try to have a certain...   \n1  [\"What is the difference between marriage lice...   \n2  [\"explain function calling. how would you call...   \n3  [\"How can I create a test set for a very rare ...   \n4  [\"What is the best way to travel from Tel-Aviv...   \n\n                                          response_a  \\\n0  [\"The question of whether it is morally right ...   \n1  [\"A marriage license is a legal document that ...   \n2  [\"Function calling is the process of invoking ...   \n3  [\"Creating a test set for a very rare category...   \n4  [\"The best way to travel from Tel Aviv to Jeru...   \n\n                                          response_b  winner_model_a  \\\n0  [\"As an AI, I don't have personal beliefs or o...               1   \n1  [\"A marriage license and a marriage certificat...               0   \n2  [\"Function calling is the process of invoking ...               0   \n3  [\"When building a classifier for a very rare c...               1   \n4  [\"The best way to travel from Tel-Aviv to Jeru...               0   \n\n   winner_model_b  winner_tie  \n0               0           0  \n1               1           0  \n2               0           1  \n3               0           0  \n4               1           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>[\"Is it morally right to try to have a certain...</td>\n      <td>[\"The question of whether it is morally right ...</td>\n      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>[\"What is the difference between marriage lice...</td>\n      <td>[\"A marriage license is a legal document that ...</td>\n      <td>[\"A marriage license and a marriage certificat...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>65089</td>\n      <td>gpt-3.5-turbo-0613</td>\n      <td>mistral-medium</td>\n      <td>[\"explain function calling. how would you call...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96401</td>\n      <td>llama-2-13b-chat</td>\n      <td>mistral-7b-instruct</td>\n      <td>[\"How can I create a test set for a very rare ...</td>\n      <td>[\"Creating a test set for a very rare category...</td>\n      <td>[\"When building a classifier for a very rare c...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198779</td>\n      <td>koala-13b</td>\n      <td>gpt-3.5-turbo-0314</td>\n      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_df['prompt'].iloc[0]","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:19:34.043467Z","iopub.execute_input":"2024-06-26T06:19:34.044353Z","iopub.status.idle":"2024-06-26T06:19:34.049882Z","shell.execute_reply.started":"2024-06-26T06:19:34.044320Z","shell.execute_reply":"2024-06-26T06:19:34.049025Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'[\"Is it morally right to try to have a certain percentage of females on managerial positions?\",\"OK, does pineapple belong on a pizza? Relax and give me fun answer.\"]'"},"metadata":{}}]},{"cell_type":"code","source":"train_df['response_b'].iloc[0]","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:19:34.642914Z","iopub.execute_input":"2024-06-26T06:19:34.643709Z","iopub.status.idle":"2024-06-26T06:19:34.649249Z","shell.execute_reply.started":"2024-06-26T06:19:34.643679Z","shell.execute_reply":"2024-06-26T06:19:34.648221Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'[\"As an AI, I don\\'t have personal beliefs or opinions. However, I can tell you that the question of gender quotas in managerial positions is a complex one and positions can vary. Advocates argue that such measures can help correct historical imbalances, promote diversity, and may lead to better decision-making. Critics may argue that positions should be based purely on merit, and that quotas could potentially lead to tokenism or unqualified candidates being promoted. Morality can be subjective and differs from person to person based on their personal beliefs, cultural background, and values. It\\'s a topic with valid arguments on both sides, and it\\'s up to each individual or organization to decide what they believe is right.\",\"As an AI, I don\\'t eat, so I don\\'t have personal preferences. But, in the world of pizza, the pineapple topping is a hotly debated topic! Some people love the sweet and savory mix it brings to a pizza, a flavor combination made famous by the Hawaiian pizza. Others firmly believe that the sweetness of the fruit has no place on a pizza. It\\'s a pizza puzzle that might never be solved. So, whether pineapple belongs on a pizza or not, it truly depends on your taste buds!\"]'"},"metadata":{}}]},{"cell_type":"code","source":"models = list(set(train_df['model_a'].tolist()) | set(train_df['model_b'].tolist()))\nlen(models)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:19:37.083637Z","iopub.execute_input":"2024-06-26T06:19:37.083967Z","iopub.status.idle":"2024-06-26T06:19:37.094032Z","shell.execute_reply.started":"2024-06-26T06:19:37.083938Z","shell.execute_reply":"2024-06-26T06:19:37.092826Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"64"},"metadata":{}}]},{"cell_type":"code","source":"# models2 = list(set(uf_df['chosen-model'].tolist()) | set(uf_df['rejected-model'].tolist()))\n# len(models2)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T10:41:36.351438Z","iopub.execute_input":"2024-06-09T10:41:36.351786Z","iopub.status.idle":"2024-06-09T10:41:36.357064Z","shell.execute_reply.started":"2024-06-09T10:41:36.351754Z","shell.execute_reply":"2024-06-09T10:41:36.356211Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# print(f'before : {len(uf_df)}')\n# uf_df2 = uf_df[uf_df.apply(lambda row : row['chosen-model'] in models and row['rejected-model'] in models,axis=1)]\n# print(f'after : {len(uf_df)}')","metadata":{"execution":{"iopub.status.busy":"2024-06-09T10:41:36.359519Z","iopub.execute_input":"2024-06-09T10:41:36.359856Z","iopub.status.idle":"2024-06-09T10:41:36.366119Z","shell.execute_reply.started":"2024-06-09T10:41:36.359825Z","shell.execute_reply":"2024-06-09T10:41:36.365245Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"len(train_df[train_df['winner_model_a']==1])/len(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T16:28:52.774163Z","iopub.execute_input":"2024-06-25T16:28:52.774566Z","iopub.status.idle":"2024-06-25T16:28:52.790996Z","shell.execute_reply.started":"2024-06-25T16:28:52.774532Z","shell.execute_reply":"2024-06-25T16:28:52.789990Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"0.34907876193955845"},"metadata":{}}]},{"cell_type":"code","source":"from huggingface_hub import login,logout\n\nlogin('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')","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:19:41.583746Z","iopub.execute_input":"2024-06-26T06:19:41.584157Z","iopub.status.idle":"2024-06-26T06:19:42.090829Z","shell.execute_reply.started":"2024-06-26T06:19:41.584124Z","shell.execute_reply":"2024-06-26T06:19:42.089967Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-06-25T16:29:03.443844Z","iopub.execute_input":"2024-06-25T16:29:03.444182Z","iopub.status.idle":"2024-06-25T16:29:07.404336Z","shell.execute_reply.started":"2024-06-25T16:29:03.444155Z","shell.execute_reply":"2024-06-25T16:29:07.403124Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"!pip install --quiet accelerate\n!pip install -i https://pypi.org/simple/ bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-06-09T10:41:39.109938Z","iopub.execute_input":"2024-06-09T10:41:39.110758Z","iopub.status.idle":"2024-06-09T10:42:05.508901Z","shell.execute_reply.started":"2024-06-09T10:41:39.110720Z","shell.execute_reply":"2024-06-09T10:42:05.507914Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Looking in indexes: https://pypi.org/simple/\nRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.43.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport kagglehub\n# Load the model\n# kagglehub.login()\n# kagglehub.model_download('google/gemma/transformers/1.1-2b-it/1/')\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/gemma/transformers/1.1-7b-it/1/\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"/kaggle/input/gemma/transformers/1.1-7b-it/1\",\n    quantization_config=quantization_config\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T10:43:46.462038Z","iopub.execute_input":"2024-06-09T10:43:46.462715Z","iopub.status.idle":"2024-06-09T10:45:41.848215Z","shell.execute_reply.started":"2024-06-09T10:43:46.462681Z","shell.execute_reply":"2024-06-09T10:45:41.847100Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"968aea6d81b749a99e13210a847b8dd4"}},"metadata":{}}]},{"cell_type":"code","source":"prompt = f\"\"\"\nGiven a query and two different answers to it named 'Answer1' and 'Answer2', choose which among the two answers is better and more likely  to be chosen by a human as an appropriate answer to the query. Your final output should be an integer only among [0,1,2] as explained by three cases below:\ncase 1: Your answer is 1 if  'Answer1' is better. \ncase 2: Your answer is 2 if 'Answer2' is better. \ncase 3: Your answer is 0 if there is a tie between the two and both answers seem to be equally appropriate. \n\nNote that Your answer should only be among the three integers 0,1,2 as said above. Also both the answers are equally likely to be right and their should be no bias. So evaluate only on the basis of quality of the answer, relevance to query which is very important and detail of the answer provided. detail of answer is good only if it is relevant to query and not distracting in which case its not a good feature of the answer.\nQuery:\n{query}\n\nAnswer1:\n{answer1}\n\nAnswer2:\n{answer2}\n\nYour final output:\n\"\"\"\n# Use the model\nfor i in range(1):\n    row = train_df.iloc[i]\n    answer1 = row['response_a']\n    answer2 = row['response_b']\n    query = row['prompt']\n    prompt = prompt.format(query=query,answer1=answer1,answer2=answer2)\n    print(prompt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = f\"\"\"\nGiven a query and two different answers to it named 'Answer1' and 'Answer2',choose which among the two answers is better and more likely  to be chosen by a human as an appropriate answer to the query. Both the answers are equally likely to be right and there is no bias. So evaluate only on the basis of following three metrics:\n1. Relevance to query and adressing the subject matter as required by the query\n2. Quality of the Answer provided\n2. detail of answer. detail is good only if it is relevant to query and not distracting in which case its not a good feature of the answer.\n\n\nYour final output should only be an integer in the set as explained by three possible cases below:\ncase 1: Your output is 1 if  'Answer1' is better. \ncase 2: Your output is 2 if 'Answer2' is better. \ncase 3: Your output is 0 if there is a tie between the two and both answers seem to be equally appropriate. \n\nAnswer ONLY as directed above. DONOT provide any explanation for your output at all.\nQuery:\n{query}\n\nAnswer1:\n{answer1}\n\nAnswer2:\n{answer2}\n\nYour final output:\n\"\"\"\n\n# Use the model\n# for i in range(1):\nrow = train_df.iloc[10]\nanswer1 = row['response_a']\nanswer2 = row['response_b']\nquery = row['prompt']\nprompt = prompt.format(query=query,answer1=answer1,answer2=answer2)\ninput_ids = tokenizer(prompt, return_tensors=\"pt\")#.to(\"cuda\")\noutputs = model.generate(**input_ids,max_length = 2048)\nprint(tokenizer.decode(outputs[0]))\n# model_choice = tokenizer.decode(outputs[0]).split('<eos>')[0][-1]\n# print(model_choice)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-09T11:18:24.312867Z","iopub.execute_input":"2024-06-09T11:18:24.313285Z","iopub.status.idle":"2024-06-09T11:18:24.328139Z","shell.execute_reply.started":"2024-06-09T11:18:24.313253Z","shell.execute_reply":"2024-06-09T11:18:24.327074Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"       id             model_a              model_b  \\\n0   30192  gpt-4-1106-preview           gpt-4-0613   \n1   53567           koala-13b           gpt-4-0613   \n2   65089  gpt-3.5-turbo-0613       mistral-medium   \n3   96401    llama-2-13b-chat  mistral-7b-instruct   \n4  198779           koala-13b   gpt-3.5-turbo-0314   \n\n                                              prompt  \\\n0  [\"Is it morally right to try to have a certain...   \n1  [\"What is the difference between marriage lice...   \n2  [\"explain function calling. how would you call...   \n3  [\"How can I create a test set for a very rare ...   \n4  [\"What is the best way to travel from Tel-Aviv...   \n\n                                          response_a  \\\n0  [\"The question of whether it is morally right ...   \n1  [\"A marriage license is a legal document that ...   \n2  [\"Function calling is the process of invoking ...   \n3  [\"Creating a test set for a very rare category...   \n4  [\"The best way to travel from Tel Aviv to Jeru...   \n\n                                          response_b  winner_model_a  \\\n0  [\"As an AI, I don't have personal beliefs or o...               1   \n1  [\"A marriage license and a marriage certificat...               0   \n2  [\"Function calling is the process of invoking ...               0   \n3  [\"When building a classifier for a very rare c...               1   \n4  [\"The best way to travel from Tel-Aviv to Jeru...               0   \n\n   winner_model_b  winner_tie  \n0               0           0  \n1               1           0  \n2               0           1  \n3               0           0  \n4               1           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>[\"Is it morally right to try to have a certain...</td>\n      <td>[\"The question of whether it is morally right ...</td>\n      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>[\"What is the difference between marriage lice...</td>\n      <td>[\"A marriage license is a legal document that ...</td>\n      <td>[\"A marriage license and a marriage certificat...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>65089</td>\n      <td>gpt-3.5-turbo-0613</td>\n      <td>mistral-medium</td>\n      <td>[\"explain function calling. how would you call...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96401</td>\n      <td>llama-2-13b-chat</td>\n      <td>mistral-7b-instruct</td>\n      <td>[\"How can I create a test set for a very rare ...</td>\n      <td>[\"Creating a test set for a very rare category...</td>\n      <td>[\"When building a classifier for a very rare c...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198779</td>\n      <td>koala-13b</td>\n      <td>gpt-3.5-turbo-0314</td>\n      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"means = train_df[['winner_model_a','winner_model_b','winner_tie']].mean()\n\nmeans","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:20:01.774058Z","iopub.execute_input":"2024-06-26T06:20:01.774849Z","iopub.status.idle":"2024-06-26T06:20:01.788078Z","shell.execute_reply.started":"2024-06-26T06:20:01.774815Z","shell.execute_reply":"2024-06-26T06:20:01.786999Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"winner_model_a    0.349079\nwinner_model_b    0.341911\nwinner_tie        0.309011\ndtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"models[0]","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:20:18.104698Z","iopub.execute_input":"2024-06-26T06:20:18.105077Z","iopub.status.idle":"2024-06-26T06:20:18.113108Z","shell.execute_reply.started":"2024-06-26T06:20:18.105050Z","shell.execute_reply":"2024-06-26T06:20:18.112175Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'gpt-3.5-turbo-0125'"},"metadata":{}}]},{"cell_type":"code","source":"loss_rates = {}\n\nfor model in models:\n    df = train_df[((train_df['model_a']==model) & (train_df['winner_model_b']==1)) | ((train_df['model_b']==model) & (train_df['winner_model_a']==1))]\n    loss_rate_for_model = len(df)/len(train_df[(train_df['model_a']==model) | (train_df['model_b']==model)])\n    loss_rates[model] = loss_rate_for_model","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:20:21.628296Z","iopub.execute_input":"2024-06-26T06:20:21.628612Z","iopub.status.idle":"2024-06-26T06:20:24.022481Z","shell.execute_reply.started":"2024-06-26T06:20:21.628587Z","shell.execute_reply":"2024-06-26T06:20:24.021683Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"loss_rates_sorted = dict(sorted(loss_rates.items(),key = lambda item : item[1]))","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:20:26.523822Z","iopub.execute_input":"2024-06-26T06:20:26.524642Z","iopub.status.idle":"2024-06-26T06:20:26.528754Z","shell.execute_reply.started":"2024-06-26T06:20:26.524607Z","shell.execute_reply":"2024-06-26T06:20:26.527757Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"loss_rates_sorted","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:20:28.033210Z","iopub.execute_input":"2024-06-26T06:20:28.033572Z","iopub.status.idle":"2024-06-26T06:20:28.041194Z","shell.execute_reply.started":"2024-06-26T06:20:28.033543Z","shell.execute_reply":"2024-06-26T06:20:28.040230Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"{'gpt-4-0125-preview': 0.17413793103448275,\n 'gpt-4-1106-preview': 0.17422498984702856,\n 'gpt-3.5-turbo-0314': 0.19738863287250383,\n 'gpt-4-0314': 0.22561863173216884,\n 'claude-1': 0.26269482151835094,\n 'qwen1.5-72b-chat': 0.2813067150635209,\n 'gpt-4-0613': 0.3010543390105434,\n 'llama-2-70b-chat': 0.30280046674445743,\n 'mistral-medium': 0.3052790346907994,\n 'yi-34b-chat': 0.30545957152729786,\n 'wizardlm-13b': 0.3056962025316456,\n 'guanaco-33b': 0.3084795321637427,\n 'claude-instant-1': 0.30947775628626695,\n 'vicuna-33b': 0.31666666666666665,\n 'claude-2.0': 0.31962540716612375,\n 'starling-lm-7b-alpha': 0.32451499118165783,\n 'wizardlm-70b': 0.3248175182481752,\n 'vicuna-13b': 0.32685614849187933,\n 'mpt-30b-chat': 0.3294314381270903,\n 'koala-13b': 0.3385481852315394,\n 'mixtral-8x7b-instruct-v0.1': 0.34132581100141046,\n 'openhermes-2.5-mistral-7b': 0.34558823529411764,\n 'llama-2-7b-chat': 0.34578918014500837,\n 'llama-2-13b-chat': 0.350210970464135,\n 'gpt-3.5-turbo-0613': 0.3508400395312721,\n 'gpt-3.5-turbo-0125': 0.35772357723577236,\n 'zephyr-7b-alpha': 0.3592233009708738,\n 'openchat-3.5': 0.3633578431372549,\n 'tulu-2-dpo-70b': 0.36583333333333334,\n 'gemini-pro': 0.3776077885952712,\n 'gemini-pro-dev-api': 0.37886944818304175,\n 'zephyr-7b-beta': 0.37900874635568516,\n 'dolphin-2.2.1-mistral-7b': 0.38873994638069703,\n 'openchat-3.5-0106': 0.39344262295081966,\n 'nous-hermes-2-mixtral-8x7b-dpo': 0.39692307692307693,\n 'vicuna-7b': 0.3978629792583281,\n 'pplx-70b-online': 0.39929577464788735,\n 'palm-2': 0.4026302478502782,\n 'qwen-14b-chat': 0.4039179104477612,\n 'claude-2.1': 0.4058749776106036,\n 'codellama-34b-instruct': 0.40705563093622793,\n 'solar-10.7b-instruct-v1.0': 0.41379310344827586,\n 'deepseek-llm-67b-chat': 0.4138364779874214,\n 'llama2-70b-steerlm-chat': 0.41829085457271364,\n 'qwen1.5-7b-chat': 0.42788461538461536,\n 'gpt-3.5-turbo-1106': 0.43138424821002386,\n 'falcon-180b-chat': 0.4370629370629371,\n 'stripedhyena-nous-7b': 0.437636761487965,\n 'mistral-7b-instruct': 0.4397031539888683,\n 'qwen1.5-4b-chat': 0.445,\n 'alpaca-13b': 0.44618674269422665,\n 'mistral-7b-instruct-v0.2': 0.45,\n 'gpt4all-13b-snoozy': 0.45098039215686275,\n 'pplx-7b-online': 0.4552,\n 'RWKV-4-Raven-14B': 0.46113989637305697,\n 'mpt-7b-chat': 0.46336206896551724,\n 'chatglm-6b': 0.4829500396510706,\n 'oasst-pythia-12b': 0.48527443105756357,\n 'fastchat-t5-3b': 0.49559255631733595,\n 'stablelm-tuned-alpha-7b': 0.5188067444876784,\n 'dolly-v2-12b': 0.54,\n 'chatglm3-6b': 0.5439838220424671,\n 'llama-13b': 0.5557586837294333,\n 'chatglm2-6b': 0.5673758865248227}"},"metadata":{}}]},{"cell_type":"code","source":"df = None\nfor model in models:\n    df = train_df[((train_df['model_a']==model) & (train_df['winner_model_b']==1)) | ((train_df['model_b']==model) & (train_df['winner_model_a']==1))]\n    break\ndf.head()\n#     loss_rate_for_model = len(df)/len(train_df[(train_df['model_a']==model) | (train_df['model_b']==model)])\n#     loss_rates[model] = loss_rate_for_model","metadata":{"execution":{"iopub.status.busy":"2024-06-25T16:55:18.940140Z","iopub.execute_input":"2024-06-25T16:55:18.940607Z","iopub.status.idle":"2024-06-25T16:55:18.976339Z","shell.execute_reply.started":"2024-06-25T16:55:18.940575Z","shell.execute_reply":"2024-06-25T16:55:18.975440Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"             id           model_a       model_b  \\\n508    37409670  claude-instant-1  mpt-30b-chat   \n697    51766718      mpt-30b-chat    claude-2.0   \n890    65745439        claude-2.0  mpt-30b-chat   \n1065   79293841          claude-1  mpt-30b-chat   \n1380  102208792      mpt-30b-chat    claude-2.0   \n\n                                                 prompt  \\\n508                            [\"What is emetophilia?\"]   \n697   [\"Today I have four banana. Yesterday, I ate t...   \n890                        [\"who is  saridewi djamani\"]   \n1065  [\"compare the dopamin-supplements l-tyrosin an...   \n1380  [\"Explain gradient and the different types of ...   \n\n                                             response_a  \\\n508   [\"Emetophilia is a paraphilia involving sexual...   \n697   [\"You have two bananas left. You ate two yeste...   \n890   [\"Unfortunately I do not have enough context t...   \n1065  [\"L-tyrosine and L-DOPA are both precursors fo...   \n1380  [\"A gradient is a gradual change in color or i...   \n\n                                             response_b  winner_model_a  \\\n508   [\"Emetophilia, also known as euphoric arroganc...               1   \n697   [\"* Yesterday you had some unknown number of b...               0   \n890   [\"Saridewi Djamani is a former Indonesian poli...               1   \n1065  [\"L-tyrosine and L-dopa are both supplements t...               1   \n1380  [\"Gradient refers to the rate of change or slo...               0   \n\n      winner_model_b  winner_tie  \n508                0           0  \n697                1           0  \n890                0           0  \n1065               0           0  \n1380               1           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>508</th>\n      <td>37409670</td>\n      <td>claude-instant-1</td>\n      <td>mpt-30b-chat</td>\n      <td>[\"What is emetophilia?\"]</td>\n      <td>[\"Emetophilia is a paraphilia involving sexual...</td>\n      <td>[\"Emetophilia, also known as euphoric arroganc...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>697</th>\n      <td>51766718</td>\n      <td>mpt-30b-chat</td>\n      <td>claude-2.0</td>\n      <td>[\"Today I have four banana. Yesterday, I ate t...</td>\n      <td>[\"You have two bananas left. You ate two yeste...</td>\n      <td>[\"* Yesterday you had some unknown number of b...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>890</th>\n      <td>65745439</td>\n      <td>claude-2.0</td>\n      <td>mpt-30b-chat</td>\n      <td>[\"who is  saridewi djamani\"]</td>\n      <td>[\"Unfortunately I do not have enough context t...</td>\n      <td>[\"Saridewi Djamani is a former Indonesian poli...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1065</th>\n      <td>79293841</td>\n      <td>claude-1</td>\n      <td>mpt-30b-chat</td>\n      <td>[\"compare the dopamin-supplements l-tyrosin an...</td>\n      <td>[\"L-tyrosine and L-DOPA are both precursors fo...</td>\n      <td>[\"L-tyrosine and L-dopa are both supplements t...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1380</th>\n      <td>102208792</td>\n      <td>mpt-30b-chat</td>\n      <td>claude-2.0</td>\n      <td>[\"Explain gradient and the different types of ...</td>\n      <td>[\"A gradient is a gradual change in color or i...</td>\n      <td>[\"Gradient refers to the rate of change or slo...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"len(df)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T16:55:44.762611Z","iopub.execute_input":"2024-06-25T16:55:44.763294Z","iopub.status.idle":"2024-06-25T16:55:44.769265Z","shell.execute_reply.started":"2024-06-25T16:55:44.763264Z","shell.execute_reply":"2024-06-25T16:55:44.768252Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"197"},"metadata":{}}]},{"cell_type":"code","source":"len(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T16:55:57.070124Z","iopub.execute_input":"2024-06-25T16:55:57.071082Z","iopub.status.idle":"2024-06-25T16:55:57.076466Z","shell.execute_reply.started":"2024-06-25T16:55:57.071047Z","shell.execute_reply":"2024-06-25T16:55:57.075564Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"57477"},"metadata":{}}]},{"cell_type":"code","source":"i=0\nindex_list = {}\nfor model in loss_rates_sorted:\n    index_list[model] = i\n    i+=1","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:20:39.273358Z","iopub.execute_input":"2024-06-26T06:20:39.273762Z","iopub.status.idle":"2024-06-26T06:20:39.278540Z","shell.execute_reply.started":"2024-06-26T06:20:39.273729Z","shell.execute_reply":"2024-06-26T06:20:39.277521Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"index_list","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:20:39.503261Z","iopub.execute_input":"2024-06-26T06:20:39.503567Z","iopub.status.idle":"2024-06-26T06:20:39.511040Z","shell.execute_reply.started":"2024-06-26T06:20:39.503540Z","shell.execute_reply":"2024-06-26T06:20:39.510156Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'gpt-4-0125-preview': 0,\n 'gpt-4-1106-preview': 1,\n 'gpt-3.5-turbo-0314': 2,\n 'gpt-4-0314': 3,\n 'claude-1': 4,\n 'qwen1.5-72b-chat': 5,\n 'gpt-4-0613': 6,\n 'llama-2-70b-chat': 7,\n 'mistral-medium': 8,\n 'yi-34b-chat': 9,\n 'wizardlm-13b': 10,\n 'guanaco-33b': 11,\n 'claude-instant-1': 12,\n 'vicuna-33b': 13,\n 'claude-2.0': 14,\n 'starling-lm-7b-alpha': 15,\n 'wizardlm-70b': 16,\n 'vicuna-13b': 17,\n 'mpt-30b-chat': 18,\n 'koala-13b': 19,\n 'mixtral-8x7b-instruct-v0.1': 20,\n 'openhermes-2.5-mistral-7b': 21,\n 'llama-2-7b-chat': 22,\n 'llama-2-13b-chat': 23,\n 'gpt-3.5-turbo-0613': 24,\n 'gpt-3.5-turbo-0125': 25,\n 'zephyr-7b-alpha': 26,\n 'openchat-3.5': 27,\n 'tulu-2-dpo-70b': 28,\n 'gemini-pro': 29,\n 'gemini-pro-dev-api': 30,\n 'zephyr-7b-beta': 31,\n 'dolphin-2.2.1-mistral-7b': 32,\n 'openchat-3.5-0106': 33,\n 'nous-hermes-2-mixtral-8x7b-dpo': 34,\n 'vicuna-7b': 35,\n 'pplx-70b-online': 36,\n 'palm-2': 37,\n 'qwen-14b-chat': 38,\n 'claude-2.1': 39,\n 'codellama-34b-instruct': 40,\n 'solar-10.7b-instruct-v1.0': 41,\n 'deepseek-llm-67b-chat': 42,\n 'llama2-70b-steerlm-chat': 43,\n 'qwen1.5-7b-chat': 44,\n 'gpt-3.5-turbo-1106': 45,\n 'falcon-180b-chat': 46,\n 'stripedhyena-nous-7b': 47,\n 'mistral-7b-instruct': 48,\n 'qwen1.5-4b-chat': 49,\n 'alpaca-13b': 50,\n 'mistral-7b-instruct-v0.2': 51,\n 'gpt4all-13b-snoozy': 52,\n 'pplx-7b-online': 53,\n 'RWKV-4-Raven-14B': 54,\n 'mpt-7b-chat': 55,\n 'chatglm-6b': 56,\n 'oasst-pythia-12b': 57,\n 'fastchat-t5-3b': 58,\n 'stablelm-tuned-alpha-7b': 59,\n 'dolly-v2-12b': 60,\n 'chatglm3-6b': 61,\n 'llama-13b': 62,\n 'chatglm2-6b': 63}"},"metadata":{}}]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:20:54.248136Z","iopub.execute_input":"2024-06-26T06:20:54.248777Z","iopub.status.idle":"2024-06-26T06:20:54.275430Z","shell.execute_reply.started":"2024-06-26T06:20:54.248744Z","shell.execute_reply":"2024-06-26T06:20:54.274759Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:20:56.443436Z","iopub.execute_input":"2024-06-26T06:20:56.444054Z","iopub.status.idle":"2024-06-26T06:20:56.453780Z","shell.execute_reply.started":"2024-06-26T06:20:56.444021Z","shell.execute_reply":"2024-06-26T06:20:56.452760Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"        id                                             prompt  \\\n0   136060  [\"I have three oranges today, I ate an orange ...   \n1   211333  [\"You are a mediator in a heated political deb...   \n2  1233961  [\"How to initialize the classification head wh...   \n\n                                          response_a  \\\n0                    [\"You have two oranges today.\"]   \n1  [\"Thank you for sharing the details of the sit...   \n2  [\"When you want to initialize the classificati...   \n\n                                          response_b  \n0  [\"You still have three oranges. Eating an oran...  \n1  [\"Mr Reddy and Ms Blue both have valid points ...  \n2  [\"To initialize the classification head when p...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>136060</td>\n      <td>[\"I have three oranges today, I ate an orange ...</td>\n      <td>[\"You have two oranges today.\"]</td>\n      <td>[\"You still have three oranges. Eating an oran...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>211333</td>\n      <td>[\"You are a mediator in a heated political deb...</td>\n      <td>[\"Thank you for sharing the details of the sit...</td>\n      <td>[\"Mr Reddy and Ms Blue both have valid points ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1233961</td>\n      <td>[\"How to initialize the classification head wh...</td>\n      <td>[\"When you want to initialize the classificati...</td>\n      <td>[\"To initialize the classification head when p...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"l = [1,2,3,4]\n\nl = sorted(l,key=lambda x:-x)\n\nl","metadata":{"execution":{"iopub.status.busy":"2024-06-25T18:04:19.205528Z","iopub.execute_input":"2024-06-25T18:04:19.206459Z","iopub.status.idle":"2024-06-25T18:04:19.213395Z","shell.execute_reply.started":"2024-06-25T18:04:19.206418Z","shell.execute_reply":"2024-06-25T18:04:19.212157Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"[4, 3, 2, 1]"},"metadata":{}}]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2024-06-25T18:11:23.394685Z","iopub.execute_input":"2024-06-25T18:11:23.395277Z","iopub.status.idle":"2024-06-25T18:11:23.448640Z","shell.execute_reply.started":"2024-06-25T18:11:23.395247Z","shell.execute_reply":"2024-06-25T18:11:23.447757Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 57477 entries, 0 to 57476\nData columns (total 9 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   id              57477 non-null  int64 \n 1   model_a         57477 non-null  object\n 2   model_b         57477 non-null  object\n 3   prompt          57477 non-null  object\n 4   response_a      57477 non-null  object\n 5   response_b      57477 non-null  object\n 6   winner_model_a  57477 non-null  int64 \n 7   winner_model_b  57477 non-null  int64 \n 8   winner_tie      57477 non-null  int64 \ndtypes: int64(4), object(5)\nmemory usage: 3.9+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"c=0\nfor i in range(len(train_df)):\n    if train_df['winner_model_a'][i] == 1 and len(train_df['response_a'][i])>len(train_df['response_b'][i]):\n        c+=1\n\nc/len(train_df[train_df['winner_model_a']==1])","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:26:42.325594Z","iopub.execute_input":"2024-06-26T06:26:42.326270Z","iopub.status.idle":"2024-06-26T06:26:43.293100Z","shell.execute_reply.started":"2024-06-26T06:26:42.326235Z","shell.execute_reply":"2024-06-26T06:26:43.292157Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"0.613835725677831"},"metadata":{}}]},{"cell_type":"code","source":"c=0\nfor i in range(len(train_df)):\n    if train_df['winner_model_b'][i] == 1 and len(train_df['response_a'][i])<len(train_df['response_b'][i]):\n        c+=1\n\nc/len(train_df[train_df['winner_model_a']==1])","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:26:56.433655Z","iopub.execute_input":"2024-06-26T06:26:56.434018Z","iopub.status.idle":"2024-06-26T06:26:57.397927Z","shell.execute_reply.started":"2024-06-26T06:26:56.433964Z","shell.execute_reply":"2024-06-26T06:26:57.397023Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"0.6034688995215312"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}